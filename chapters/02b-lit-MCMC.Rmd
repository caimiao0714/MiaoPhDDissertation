
Predictive models
-----------------

### Bayesian models

In contrast to traditional frequentist models that view parameters as unknown but fixed values, Bayesian models view parameters as random variables that have probability distributions [@gelman2013bayesian].

\begin{equation}
\begin{split}
p(\theta | \mathbf{X}) & = \frac{p(\theta)p(\mathbf{X}|\theta)}{p(\mathbf{X})} \\
 & = \frac{p(\theta)p(\mathbf{X}|\theta )}{\int p(\theta)p(\mathbf{X}|\theta)d\theta}
(\#eq:bayes)
\end{split}
\end{equation}

Where $p(\theta | \mathbf{X})$ is the posterior distribution of the parameters, $p(\theta)$ is the prior distribution of the parameters, $p(\mathbf{X}|\theta)$ is the likelihood function, and $\int p(\theta)p(\mathbf{X}|\theta)d\theta$ is a normalizing constant.

The prior distribution reflects the researcher's subjective belief on the parameters before observing any data. The likelihood function reflects the data generating process that gives rise to the data observed. The posterior distribution is an unconditional distribution that includes the researcher's belief on the parameters after observing the data. The prior and likelihood function are straightforward since they both have analytical forms. The most tricky part of Bayesian inference is the normalizing constant in the denominator [@gelman2013bayesian]. 

The normalizing constant need to make the posterior distribution integrate to one since the posterior is supposed to be a probability density distribution. When there are more than two parameters in the model, the normalizing constant often becomes intractable since it involves integration in multiple dimensions. Modern Bayesian inference often uses numerical methods, such as Markov chain Monte Carlo (MCMC) methods, to calculate or approximate this constant.


**Advantages of Bayesian inference**

**Posterior predictive distribution**.

### Hierarchical models

Driver-focused statistical models [@cantor2010driver].

partial pooling

### Markov chain Monte Carlo (MCMC)

In modern statistics, Bayesian inference almost indispensably relies on Markov chain Monte Carlo (MCMC) sampling to overcome the intractable denominator in the Bayes Theroem (Equation \@ref(eq:bayes)). A **Monte Carlo simulation** is a technique to understand a target distribution by generating a large amount of random values from that distribution [@kruschke2014doing]. A **Markov chain** has the property that the probability distribution of the observation $i$ only depends on the previous observation $i-1$, not on any one prior to observation $i-1$, as demonstrated in Equation \@ref(eq:markovchain). 
\begin{equation}
P\left(X_{i}=x_{i} | X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{i-1}=x_{i-1}\right)=P\left(X_{i}=x_{i} | X_{i-1}=x_{i-1}\right)
(\#eq:markovchain)
\end{equation}

Integrating Markov chains and Monte Carlo simulations, the MCMC method can characterize an unknown unconditional distribution without knowing its all mathematical properties by sampling from the distribution [@van2018simple]. It has been widely applied in multiple fields such as statistics, physics, chemistry, and computer science [@craiu2014bayesian].  The most notable application of MCMC is probably in Bayesian inference, in which it has been used to draw samples from the posterior distribution and calculate relevant statistics (such as mean, standard deviation, and intervals). 

The first proposal of using MCMC dates back to the paper by @metropolis1953equation, in which they tried to solve an intractable integral with a random walk MCMC. The Metropolis algorithm starts with a randomly defined initial value of the parameter $\theta$. From a pre-defined symmetric proposal probability distribution  $p(\theta | \mathbf{x})$, it then draw a proposal parameter value $\theta^{(\text{prop})}$, which only depends on the current parameter value $\theta^{(t)}$. This proposal value will be accepted with the probability of $\alpha$ defined in Equation \@ref(eq:metropolisalpha).
\begin{equation}
\alpha = \min\bigg(1, \frac{p(\theta^{(\text{prop})}|\mathbf{x})}{p(\theta^{(t)}|\mathbf{x})}\bigg)
(\#eq:metropolisalpha)
\end{equation}

This proposal and acceptance with probability steps will be iterated for a pre-define number of times. When the Metropolis algorithm reaches a steady state, these proposal values are random values drawn from the posterior distribution of parameter $\theta$, which can be used to describe and characterize the posterior distribution.


After decades of successful empirical trials in physics, @hastings1970monte proposed a more generalized form of the Metropolis algorithm, in which the proposal distribution can be arbitrary, but the acceptance probability $\alpha^\star$ is modified as shown in Equation \@ref(eq:MHalpha). This Metropolis-Hasting (MH) algorithm is the most classic and widely-known MCMC algorithm used in multiple fields.

Let $p(\mathbf{\theta|X})$ be the posterior distribution we want to know, then the *Metropolis-Hasting algorithm* is:
  
1. Let $\theta^{(1)}$ denote an initial value for the continuous state Markov chain,
2. Set $t = 1$,
3. Let $q$ be the proposal density which can depend on the current state $\theta^{(t)}$. Simulate one observation $\theta^{(\text{prop})}$ from $q(\theta^{(\text{prop})} | \theta^{(t)})$,
4. Compute the following probability:
\begin{equation}
\alpha^\star = \min\bigg(1, \frac{p\left(\theta^{(\text{prop})} | \boldsymbol{x}\right)}{p\left(\theta^{(t)} | \boldsymbol{x}\right)} \frac{q\left(\theta^{(t)} | \theta^{(\text{prop})}\right)}{q\left(\theta^{(\text{prop})} | \theta^{(t)}\right)}\bigg)
(\#eq:MHalpha)
\end{equation}
5. Set $\theta^{(t+1)} = \theta^{(\text{prop})}$ with the probability of $\alpha^\star$; otherwise set $\theta^{(t+1)} = \theta^{(t)}$. Set $t \leftarrow t + 1$ and return to 3 until the desired number of iterations is reached.


Although the M-H algorithm is simple and powerful for performing MCMC, its performance highly depends on the choice of the proposal distribution. When there are a few parameters in the model and the proposal distribution is not well-designed, the M-H algorithm will have a very low acceptance rate, which makes the M-H algorithm very inefficient. In view of this issue, Gibbs sampler was proposed with the idea that the proposed values are always accepted and each parameter is updated one at a time by generating samples from the conditional distributions [@geman1987stochastic; @gelfand1990sampling; @lambert2018student]. The development of the software *Bayesian inference Using Gibbs Sampler (BUGS)* [@lunn2000winbugs; @lunn2009bugs] was critical in increasing the popularity of applied Bayesian analyses considering its support for a wide variety of statistical distributions, automatic application of the Gibbs Sampler, and numerous textbooks, tutorials and discussion.

Suppose $\mathbf{\theta} = [\theta_1, \theta_2, \cdots, \theta_k]$ is a $k$-dimensional parameter. Let $\mathbf{X}$ denote the data. The *Gibss sampling* algorithm is then:
  
1. Begin with an estimate $\mathbf{\theta}^{(0)} = [\theta_1^{(0)}, \theta_2^{(0)}, \cdots , \theta_k^{(0)}]$ in the parameter space,
2. Set $t = 1$,
3. Simulate $\theta_1^{(t)}$ from $p(\theta_1|\theta_2^{(t-1)}, \theta_3^{(t-1)},\cdots , \theta_k^{(t-1)}, \mathbf{X})$,
4. Simulate $\theta_2^{(t)}$ from $p(\theta_2|\theta_1^{(t)}, \theta_3^{(t-1)},\cdots , \theta_k^{(t-1)}, \mathbf{X})$,
5. $\cdots$,
6. Simulate $\theta_k^{(t)}$ from $p(\theta_k|\theta_1^{(t)}, \theta_3^{(t)},\cdots , \theta_{k-1}^{(t)}, \mathbf{X})$,
7. Set $t \leftarrow t + 1$ and repeat steps $3-6$ for a pre-specified number of iterations and make sure the Gibbs sampler reaches the steady state for a sufficient number of iterations.


The generality of the M-H algorithm and Gibbs sampler and the simplicity in software packages in `R` or `BUGS` help them gain popularity among applied researchers in the recent 30 years. However, as more and more data are available in applied field, the performance of the two most popular MCMC methods has been widely criticized [@betancourt2019convergence]. The performance of the M-H algorithm crucially depends on the proposal distribution. A efficient proposal distribution in M-H algorithm should generate random draws with less auto-correlation, which enables more effective exploration of the parameter space [@quiroz2015bayesian]. On the other hand, the performance of the Gibss Sampler crucially depends on the parameter structure. If there is a significant correlation between parameter estimates, the Gibbs Sampler will become very inefficient as the geometry of the distribution is not aligned with the stepping directions of each sampler [@lambert2018student].


Scalable Bayesian models
------------------------

Recent ten years witnessed an explosive growth of data size and dimensionality.  This poses a major challenge to Bayesian methods using MCMC. Traditional MCMC algorithm need to evaluate the entire data at each step of iteration, which could be expensive for computation in the case of tall data [@bardenet2017markov]. In applied analysis, researchers often need to set  thousands of iterations to reach stable posterior distribution, which takes hours or days to implement a single model. Besides, when the researchers have high dimensional data where high-probability regions are concentrated on a extremely limited region of sample space, it would very hard for random-walk MCMC to generate samples from these small regions [@barp2018geometry]. Hierarchical models even complicates this issue by adding random parameters for each subgroup, which further grows the dimensionality of parameter space. Furthermore, when there is high correlation between different parameters that often occur in the case of many parameters, neither the M-H algorithm or Gibbs sampler can efficiently generate samples from the posterior distribtion. All the aforementioned problems motivates researchers in different fields to develop different scalable algorithms to make Bayesian inference for big data.

### Hamiltonian Monte Carlo (HMC)

The M-H algorithm and Gibbs sampler can be very inefficient in big data settings as a consequence of sparse high-density parameter space, high costs of evaluating the entire data at each step, or a high correlation between parameters. Originally proposed by @duane1987hybrid with the name of Hybrid Monte Carlo, the Hamiltonian Monte Carlo (HMC) modifies the random-walk behavior in M-H algorithm into a deterministic one by adding auxiliary momentum parameters $p_n$, thus more efficiently explores the high-density regions in big data settings compared to the traditional M-H algorithm or the Gibbs sampler [@betancourt2017conceptual; @wang2019hamiltonian]. Although HMC was originally proposed in 1987 [@duane1987hybrid], it is only widely adopted by applied researchers in the recent five years, thanks to the development of the No-U-Turn Sampler (NUTS) [@hoffman2014no] and the statistical programming language `Stan` [@carpenter2017stan].

Let $\boldsymbol{q}$ denote the position vector and $\boldsymbol{p}$ denote the momentum vector in the conservative dynamics physics system. Note that $\boldsymbol{q}$ and $\boldsymbol{q}$ must have the same length. The combination $(\boldsymbol{q}, \boldsymbol{p})$ then defines a position-momentum phase space, which can be calculated using the conditional distribution [@neal2011mcmc; @betancourt2017conceptual]:
$$\pi(\boldsymbol{p}, \boldsymbol{q}) = \pi(\boldsymbol{p}|\boldsymbol{q})\pi(\boldsymbol{q})$$

This joint distribution can also be defined in terms of the *Hamiltonian*:
$$\pi(\boldsymbol{p}, \boldsymbol{q}) = e^{-H(\boldsymbol{p}, \boldsymbol{q})}$$
After a little bit of transformation, we have:
\begin{equation}
\begin{aligned}
H(\boldsymbol{p}, \boldsymbol{q}) &= -\log \pi(\boldsymbol{p}, \boldsymbol{q})\\
&= -\log\pi(\boldsymbol{p}|\boldsymbol{q}) - \log\pi(\boldsymbol{q})\\
&= K(\boldsymbol{p}, \boldsymbol{q}) + V(\boldsymbol{q})
(\#eq:hamiltonian)
\end{aligned}
\end{equation}

In the perspective of physics, the *Hamiltonian* $H(\boldsymbol{p}, \boldsymbol{q})$ is the total energy of the system, which composes of two parts: *kinetic energy* $K(\boldsymbol{p}, \boldsymbol{q})$ and *potential energy* $V(\boldsymbol{q})$. Note that the potential energy $V(\boldsymbol{q}) = -\log\pi(\boldsymbol{q})$ is essentially the negative log of the posterior distribution of the parameter posterior density $\boldsymbol{q}$.

In a static system, the Hamiltonian is a constant. The evolution of this system is governed by the *Hamiltonian equations*:
\begin{equation}
\begin{aligned}
\frac{d \boldsymbol{q}}{dt} &= \frac{\partial H}{\boldsymbol{p}} = \frac{\partial K}{\boldsymbol{p}}\\
\frac{d \boldsymbol{p}}{dt} &= -\frac{\partial H}{\boldsymbol{q}} = -\frac{\partial K}{\boldsymbol{q}} - \frac{\partial V}{\boldsymbol{q}}
(\#eq:hamiltonianequation)
\end{aligned}
\end{equation}

It turns out that we can randomly generate high density proposals in the parameters space by taking advantage of the Hamiltonian system. Here is the general idea if the *HMC algorithm* [@lambert2018student]: 

1. Let $\theta^{(0)}$ denote a random initial value from a proposal distribution,
2. Set $t = 1$,
3. Generate a random initial momentum $m$ from a proposal distribution (typically a multivariate normal distribution),
4. Use the leapfrog algorithm to solve the trajectory moving over the high-density posterior parameter space under the Hamiltonian mechanism for a time period,
5. Calculate the new momentum $m^\prime$ and new position $\theta^{(\text{prop})}$
6. Compute the following probability:
\begin{equation}
\alpha^H = \min\bigg(1, \frac{p\left(\theta^{(\text{prop})} | \boldsymbol{x}\right)p(\theta^{(\text{prop})})}{p\left(\theta^{(t)} | \boldsymbol{x}\right)p(\theta^{(t)})} \frac{q(m^\prime)}{q(m)}\bigg)
(\#eq:hmc)
\end{equation}
7. Set $\theta^{(t+1)} = \theta^{(\text{prop})}$ with the probability of $\alpha^H$; otherwise set $\theta^{(t+1)} = \theta^{(t)}$. Set $t \leftarrow t + 1$ and return to 3 until the desired number of iterations is reached.

The HMC is essentially an improved form of M-H algorithm by using the Hamiltonian to generate effective proposals instead of naive random-walk and revised form of the acceptance probability (Equation \@ref(hmc)).

Two parameters need to be tuned when implementing the HMC: step size $\epsilon$ and the optimal trajectory length $T$. The optimal trajectory length is the product of the number of steps $L$ and step size $\epsilon$ [@neal2011mcmc; @monnahan2017faster]. The step size $\epsilon$ decides how similarly the sympletic methods (typically the leapfrog algorithm) imitates the true unnormalized posterior density. If $\epsilon$ is too small, it will take a lot of steps for the leapfrog algorithm to explore the posterior space. If $\epsilon$ is too big, the leapfrog algorithm will loop around and return back to close to its original place. The trajectory length $T = \epsilon L$, which need to be tuned in similar style with $\epsilon$: if $L$ is too short, it will be hard to simulate distant proposal and the algorithm is inefficient; if $L$ is too long, the trajectory will loop back and become computationally inefficient. Hand tuning these two parameters was the major obstacle to implement HMC for applied researchers.

The No-U-Turn Sampler (NUTS) proposed by @hoffman2014no solved the difficulty of hand tuning $\epsilon$ and $T$ in static HMC. NUTS calculates the optimal step size $\epsilon$ and number of steps $L$ through a tree building algorithm [@monnahan2017faster]. The tree depth $k$ is defined as the number of doublings, resulting in $2^k$ leapfrog steps to build the trajectory. This $k$ is then decided by repeating the doubling iterations until the trajectory 'makes a U-turn' (loops back) or diverges (the Hamiltonian expands to infinity). Therefore, the NUTS can automatically create trajectories that can efficiently explore the high-density parameter space without having to hand tune $\epsilon$ and $T$.


### Integrated Laplace Approximation (INLA)

[@Havard2009; @Lindgren2011; @Thiago2013; @Coninck2016; @Rue2017; @Verbosio2017; @Kourounis2018]


### Subsampling MCMC


[Stochastic Gradient HMC](https://blog.csdn.net/u013841458/article/details/82495450)

[@quirozsubsampling; @gunawan2018subsampling; @quiroz2016block; @quiroz2018speeding; @quiroz2018speeding1; @dang2017hamiltonian; @quiroz2015bayesian]

@chen2014stochastic